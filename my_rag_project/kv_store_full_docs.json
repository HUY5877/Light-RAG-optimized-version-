{
  "doc-03d9a19ca90cc93066ce45a3eae66ed1": {
    "content": "bioRxiv preprint doi: https://doi.org/10.1101/2023.01.07.523036; this version posted January 7, 2023. The copyright holder for this preprint (which was not certified by peer review) is the author/funder, who ha ranted bioRxiv a license to display the preprint in perpetuity. It is made available under aCC-BY-NC 4.0 International license. \n##########################################################################\n这是一个大标题:Solving the spike sorting problem with Kilosort \nMarius Pachitariu1†, Shashwat Sridhar2 , Carsen Stringer1   \n1HHMI Janelia Research Campus,   \n2Department of Ophthalmology, University Medical Center Go¨ttingen. Go¨ttingen   \n† correspondence to pachitarium@hhmi.org \n\nMarius Pachitariu1†, Shashwat Sridhar2 , Carsen Stringer1   \n1HHMI Janelia Research Campus,   \n2Department of Ophthalmology, University Medical Center Go¨ttingen. Go¨ttingen   \n† correspondence to pachitarium@hhmi.org \n\nMarius Pachitariu1†, Shashwat Sridhar2 , Carsen Stringer1   \n1HHMI Janelia Research Campus,   \n2Department of Ophthalmology, University Medical Center Go¨ttingen. Go¨ttingen   \n† correspondence to pachitarium@hhmi.org \n\n##########################################################################\nMarius Pachitariu1†, Shashwat Sridhar2 , Carsen Stringer1   \n1HHMI Janelia Research Campus,   \n2Department of Ophthalmology, University Medical Center Go¨ttingen. Go¨ttingen   \n† correspondence to pachitarium@hhmi.org \n##########################################################################\nSpike sorting is the computational process of extracting the firing times of single neurons from recordings of local electrical fields. This is an important but hard problem in neuroscience, complicated by the nonstationarity of the recordings and the dense overlap in electrical fields between nearby neurons. To solve the spike sorting problem, we have continuously developed over the past eight years a framework known as Kilosort. This paper describes the various algorithmic steps introduced in different\n##########################################################################\nversions of Kilosort. We also report the development of Kilosort4, a new version with substantially improved performance due to new clustering algorithms inspired by graph-based approaches. To test the performance of Kilosort, we developed a realistic simulation framework which uses densely sampled electrical fields from real experiments to generate non-stationary spike waveforms and realistic noise. We find that nearly all versions of Kilosort outperform other algorithms on a variety of simulated\n##########################################################################\nconditions, and Kilosort4 performs best in all cases, correctly identifying even neurons with low amplitudes and small spatial extents in high drift conditions.这是一个大标题:Introduction \n2 Classical spike sorting frameworks require a sequence   \n3 of operations, which can be categorized into prepro  \n4 cessing, spike detection, clustering and postprocess  \n5 ing. Modern approaches have improved on these   \n6 steps by introducing new algorithms. Some frame  \n7 works [1–3] took advantage of new clustering algo  \n8 rithms such as density-based approaches [4] or ag  \n9 glomerative approaches using bimodality criteria [5].   \n10 In contrast, the original Kilosort [6] used a simple clus  \n11 tering approach (scaled K-means), but combined two   \n12 steps of the pipeline into one (spike detection $^+$ cluster  \n13 ing $\\mathbf{\\sigma}=\\mathbf{\\sigma}$ template learning) and added an extra matching   \n14 pursuit step for detecting overlapping spikes, some  \n15 times referred to as solving the “collision problem” [7]. \n\n##########################################################################\n2 Classical spike sorting frameworks require a sequence 3 of operations, which can be categorized into prepro 4 cessing, spike detection, clustering and postprocess 5 ing. Modern approaches have improved on these 6 steps by introducing new algorithms. Some frame 7 works [1–3] took advantage of new clustering algo 8 rithms such as density-based approaches [4] or ag 9 glomerative approaches using bimodality criteria [5]. 10 In contrast, the original Kilosort [6] used a simple clus 11 tering approach (scaled\n##########################################################################\nK-means), but combined two 12 steps of the pipeline into one (spike detection $^+$ cluster 13 ing $\\mathbf{\\sigma}=\\mathbf{\\sigma}$ template learning) and added an extra matching 14 pursuit step for detecting overlapping spikes, some 15 times referred to as solving the “collision problem” [7].An important consideration for these early modern algorithms was the requirement for additional human curation, as the clustering results were imperfect in many applications. Thus, algorithms like Kilosort biased the clustering process towards “over-splitting”, producing more clusters than the number of real units in the data, so that human curation would consist primarily of merges, which are substantially easier to perform than splits. To facilitate human curation of the automated results, a modern\n##########################################################################\ngraphical user interface called Phy was developed, which is now used for visualization by several of the most popular frameworks including all versions of Kilosort [8].Why was human curation still necessary for these early modern methods? One of the main reasons was the non-stationary nature of data from real experiments. The electrical field of a unit sampled by a probe, called a spike waveform, should be fixed and \n##########################################################################\n34 reproducible across long time periods. Yet in many 35 experiments, the shape of the waveform appeared to 36 change over the course of hours, and sometimes much 37 faster. The main reason for these changes was identi 38 fied as vertical probe movement or “drift”, using high 39 density electrodes [9]. Drift is primarily caused by fac 40 tors such as tissue relaxation after probe insertion and 41 animal movements during behavior. Correcting for this 42 drift resulted in substantial improvements in spike\n##########################################################################\nsort 43 ing performance. Kilosort2 used a “drift tracking” ap 44 proach for this, while Kilosort2.5 developed a stan 45 dalone drift correction method that directly modified 46 the voltage data to shift certain channels up or down 47 by appropriate distances (see Methods for drift track 48 ing, and Methods in [9] for drift correction). The drift 49 correction step has been inherited by all Kilosort ver 50 sions since 2.5.The main goal of this paper is to describe the development of Kilosort4 and demonstrate its performance. Some of the algorithmic steps in Kilosort4 are inherited from previous versions (i.e. drift correction), while others build on top of previous versions (i.e. template deconvolution), while others are completely new (i.e. the graph-based clustering approach). Except for drift correction, which was previously described in detail [9], the other algorithmic steps are not described in the literature, and we\n##########################################################################\nadd detailed descriptions in the Methods (see Table 1 for an overview). We also developed a new simulation-based framework for benchmarking spike sorting algorithms, which uses several realistic drift patterns and dense electrical fields inferred from real experiments. We show using the benchmarks that Kilosort4 performs very well and outperforms all other algorithms across a range of conditions.bioRxiv preprint doi: https://doi.org/10.1101/2023.01.07.523036; this version posted January 7, 2023. The copyright holder for this preprint (which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under aCC-BY-NC 4.0 International license. \n##########################################################################\n这张图片展示了Kilosegment (简称Kilosegment) 算法各个版本的特点，主要对比的是其在图像分割方面的技术实现和功能。\n\n**整体概览:**\n\nKilosegment 是一系列图像分割算法，由版本 1 (2016) 到版本 4 (2023) 不断演进。图片从预处理、模板反卷积（deconvolution）和后处理三个方面，对比了不同版本在这些步骤中的技术选择。\n\n**具体解释:**\n\n* **预处理:**\n    * **Filtering & Whitening (滤波与白化):** 几乎所有版本都支持，用于提高图像质量和消除噪声。\n    * **Drift Correction (漂移校正):** 从版本2开始支持，用于校正图像中的移动或变形。\n\n* **模板反卷积 (Template Deconvolution):** 核心步骤，用于重建图像细节。\n    *  所有版本都使用反卷积，版本 1 和 2 使用了初始的“scaled k-means”进行模板学习。\n    *  版本 2 和之后版本在反卷积过程中加入了“during learning”和“from residual”策略。\n\n* **后处理:**\n    * **Threshold crossing (阈值穿越):** 版本2及以后版本采用，用于将反卷积结果转换为分割结果。\n    * **Recursive Pursuit (递归搜索):** 版本 3 和 4 采用递归搜索算法，进一步优化分割结果。\n    * **Graph Clustering (图聚类):**  版本 4 引入，利用图结构进行分割优化。\n    * **Bimodality Pursuit (双峰搜索):** 版本2及以后版本采用，用于分割图像中的不同区域。\n    * **Merging Tree (合并树):** 版本4引入，用于合并分割区域。\n\n**关键点:**\n\n*  `*` 符号表示一个重要的改进或新引入的功能。\n*  图片清晰地展示了 Kilosegment 在不同版本中功能逐渐完善的过程。\n*  从 MATLAB 逐渐过渡到 Python + PyTorch 也是一个重要的变化。\n\n总而言之，这张图是Kilosegment算法版本演进的一个技术对比图，展示了它在图像分割方面的进步。\n\n##########################################################################\n这是一个大标题:68 Results \nTo be able to process the large amounts of data from modern electrophysiology, all versions of Kilosort are implemented on the GPU. Kilosort4 is the first version fully implemented in python and using the pytorch package for all its functionality, thus making the old CUDA functions obsolete [10, 11]. Pytorch allows the user to switch to a CPU backend which may be sufficiently fast for testing on small amounts of data but is not recommended for large-scale data. All versions of Kilosort take as input a binary data file, and output a set of “.npy” files that can be used for visualization in Phy [8]. To set up a Kilosort4 run, we built a pyqtgraph GUI which replicates the functionality of the Matlab GUI, and can assist users in debugging due to several diagnostic plots and summary statistics that are displayed [12] (Figure S1). \n\n##########################################################################\nTo be able to process the large amounts of data from modern electrophysiology, all versions of Kilosort are implemented on the GPU. Kilosort4 is the first version fully implemented in python and using the pytorch package for all its functionality, thus making the old CUDA functions obsolete [10, 11]. Pytorch allows the user to switch to a CPU backend which may be sufficiently fast for testing on small amounts of data but is not recommended for large-scale data. All versions of Kilosort take as input a\n##########################################################################\nbinary data file, and output a set of “.npy” files that can be used for visualization in Phy [8]. To set up a Kilosort4 run, we built a pyqtgraph GUI which replicates the functionality of the Matlab GUI, and can assist users in debugging due to several diagnostic plots and summary statistics that are displayed [12] (Figure S1).The preprocessing step in all versions of Kilosort includes temporal filtering and channel whitening (see Methods). These linear operations reduce the strong spatiotemporal correlations of the electrical background in the brain, which is mainly formed by the electrical discharge of units that are too far from the probe to be identified as single units. This step is accelerated in Kilosort4 through the use of explicit convolutions in place of a Butterworth filter. Drift correction is an additional\n##########################################################################\npreprocessing step that was introduced in Kilosort2.5 and maintained in all subsequent versions (see Methods of [9]). Unlike previous versions, Kilosort4 no longer needs to generate an intermediate file of processed data, as all preprocessing operations are fast enough to be performed on-demand.这是一个大标题:Template deconvolution \nWe refer to the spike detection and feature extraction steps jointly as “template deconvolution”. This module requires a set of templates which correspond to the average spatiotemporal waveforms of neurons in the recording. The templates are used in the matching pursuit step for detecting overlapping spikes [6]. A template deconvolution step has been used in all versions of Kilosort, but the details of the template learning have changed (see Methods). In Kilosort 3 and 4, the template deconvolution serves an extra role as a feature extraction method with background correction. \n\n##########################################################################\nWe refer to the spike detection and feature extraction steps jointly as “template deconvolution”. This module requires a set of templates which correspond to the average spatiotemporal waveforms of neurons in the recording. The templates are used in the matching pursuit step for detecting overlapping spikes [6]. A template deconvolution step has been used in all versions of Kilosort, but the details of the template learning have changed (see Methods). In Kilosort 3 and 4, the template deconvolution serves\n##########################################################################\nan extra role as a feature extraction method with background correction.The template deconvolution pipeline has the same format for both Kilosort 3 and 4 (Figure 1a). A set of initial spike waveforms are extracted from preprocessed data using a set of universal templates (Figure 1b,c). The features of these spikes are then clustered, using either the recursive pursuit algorithm from Kilosort3 (see Methods), or the graph-based algorithm from Kilosort4 (described in Figure 2). The centroids of the clusters are the “learned templates”, which are then aligned temporally (Figure\n##########################################################################\n1d). The templates are compared to each other by cross-correlation and similar templates are merged together to remove duplicates. The learned templates are then used in the matching pursuit step, which iteratively finds the best matching templates to the preprocessed data and subtracts off their contribution. The subtraction is a critical part of all matching pursuit algorithms and allows the algorithm to detect spikes that were overlapped by the subtracted ones. The final reconstruction of the data with\n##########################################################################\nthe templates is shown in Figure 1e. The residual is the difference between data and reconstruction, andbioRxiv preprint doi: https://doi.org/10.1101/2023.01.07.523036; this version posted January 7, 2023. The copyright holder for this preprint (which was not certified by peer review) is the author/funder, who nted bioRxiv a license to display the preprint in perpetuity. It is made available under aCC-BY-NC 4.0 International license. \n##########################################################################\n这张图片展示了一个神经信号处理流程，用于从数据中提取神经信号（spike）并进行分析。 下面是对各个子图的解释：\n\n* **a. 流程图：**  概述了整个数据处理流程，从原始信号处理到最终簇分析。\n* **b. 原始数据（preprocessed data）：** 展示了经过预处理后的原始神经信号波形。  横轴代表时间，纵轴代表电极通道。  颜色代表信号强度。\n* **c. 通用模板 (Universal templates)：**  展示了从原始数据中提取的通用神经波形模板。  每个小方格代表一个波形。\n* **d. 学习模板 (Learned templates)：** 展示了从原始数据中学习的神经波形模板。类似于 c，但使用了不同的提取方法。\n* **e. 重构 (Reconstruction)：** 将数据使用通用模板重构后的结果。用于评估模板的拟合程度。\n* **f. 残差 (Residual)：**  原始数据和重构数据之间的差异。  显示了哪些部分没有被模板有效捕捉。\n* **g. 特征从通用模板 (Features from universal templates)：** 用通用模板提取的特征。\n* **h. 特征从学习模板 (Features from learned templates)：** 用学习模板提取的特征。\n* **i. 特征从背景减法 (Features from background subtracted)：** 用背景减法后的特征。\n* **j. 神经信号分布 (Spike distributions)：**  在空间坐标系中显示了提取的神经信号的位置。  横轴表示深度，纵轴表示横向位置。 颜色代表了“template norm”的值，可以理解为模板匹配的强度或相似度。 显示了75,546个神经信号在空间上的分布情况。\n\n总的来说，这张图片展示了一个完整的神经信号处理流程，包括数据预处理、模板提取、特征提取和空间分布分析。\n##########################################################################\ncan be informative if the algorithm fails to find some units (Figure 1f). \n##########################################################################\n135 This template learning step from Kilosort $_{3/4}$ is dif 136 ferent from the one in Kilosort1 (see [6]), and both 137 are different from the equivalent step in Kilosort 2/2.5 138 (see Methods). Furthermore, the templates of Kilo 139 sort1 are in one-to-one correspondence with the final 140 inferred units which are exported for manual curation. 141 This correspondence is weaker in Kilosort 2/2.5, be 142 cause a post-processing step is used to perform splits 143 and merges on these templates (see\n##########################################################################\nMethods). Fi 144 nally, in Kilosort $3/4$ these templates are completely 145 discarded after being used to extract spikes. This is 146 because more powerful clustering algorithms can be 147 applied to the spike features once they have been ex 148 tracted with template deconvolution. The “corrected” 149 or deconvolved features have three additional proper 150 ties compared to the features detected with universal 151 templates or more generally detected with any clas 152 sical threshold crossing method: 1)\n##########################################################################\nthey contain all, 153 or a majority of spikes from the clustered units, eventhe ones that are overlapped by larger, bigger spikes; 2) they group spikes together by templates, which can be used to more precisely assign spikes to their best channels for batched clustering across channels; 3) they can be computed after subtraction of the background produced by all other spikes (Figure 1e). \n##########################################################################\nThese properties have a substantial effect on the features, allowing for better clustering. Figure 1g-i show the t-SNE embeddings of three different sets of features from spikes detected over a 40um stretch of a Neuropixels probe. The features computed with the learned templates with background subtraction (Figure 1i) are embedded as more uniform, Gaussian-like clusters. Without background subtraction, each cluster is surrounded by a patterned envelope of points due to the contribution of overlapping\n##########################################################################\nspikes, and these patterns can be easily mistaken for other clusters (Figure 1g,h). The visualization in Figure 1i can be used to get an impression of a small section of the data without performing any clustering at all. To visualize the distribution of spikes over a larger portion of a probe, we plot a subset of spikes at their inferred XY positions (Figure 1j). The spikes are colored according to amplitudes, which tends to be uniform for spikes from the same unit.好的，我来解释一下这张图片，它展示了一个关于“难熔单元 (refractory units)”的聚类分析和特征可视化过程。\n\n**总览**\n\n这张图的主要目的是将一组“难熔单元”进行聚类，并展示它们在空间中的分布和信号特征。这些“难熔单元”可能是在生物组织或器件中难以溶解的结构，例如细胞核或金属颗粒。\n\n**各个部分的解释:**\n\n*   **(a)**：使用邻域聚类方法（neighbor clustering）对数据进行初步划分，展示了三个聚类后的特征向量（例如主成分）。\n*   **(b)**：使用t-SNE方法对27个聚类进行降维并进行可视化，每个点代表一个“难熔单元”，颜色代表不同的聚类。\n*   **(c)**：聚类合并树。为了得到更合理的聚类，采用合并算法，调整聚类数量。\n*   **(d-e)**：展示了聚类合并过程中的一个例子，通过合并一些聚类，来优化聚类结果。\n*   **(f)**：进一步将聚类数量减少到9个，并重新可视化“难熔单元”的分布。\n*   **(g)**：显示每个聚类中“难熔单元”的波形信号。波形是用于识别和区分不同单元的重要特征。\n*   **(h)**：展示了自相关图（autocorrelation）和交叉相关图（crosscorrelation）。这些图可以帮助分析波形的相似性和时间关系。\n*   **(i)**：展示了“难熔单元”在三维空间中的位置分布，横坐标为深度，纵坐标为横向位置。\n\n**总结**\n\n这张图片详细展示了从数据聚类到可视化分析的完整流程，通过可视化和信号特征分析，我们可以更好地理解“难熔单元”的空间分布和信号特性。\n\n**提示:** 如果有具体的疑问，请告诉我，我会尽力解答。\n##########################################################################\n\n##########################################################################\n这是一个大标题:Graph-based clustering with merging trees \nWe developed two new clustering algorithms for spike features extracted by template deconvolution. In Kilosort3, we developed an algorithm that uses a recursive application of the bimodality pursuit algorithm from Kilosort2, which in turn had been developed to automatically find potential splits within clusters (see Methods). In Kilosort4 we developed a graph-based clustering method. This approach first constructs a graph of points connected to their nearest neighbors in Eu\n\n##########################################################################\nWe developed two new clustering algorithms for spike features extracted by template deconvolution. In Kilosort3, we developed an algorithm that uses a recursive application of the bimodality pursuit algorithm from Kilosort2, which in turn had been developed to automatically find potential splits within clusters (see Methods). In Kilosort4 we developed a graph-based clustering method. This approach first constructs a graph of points connected to their nearest neighbors in Eu\n##########################################################################\n189 clidean space, then constructs a cost function from 190 the graph properties to encourage the clustering of 191 nodes. A popular cost function is “modularity”, which 192 counts the number of graph edges inside a cluster 193 and compares them to the expected number of edges 194 from a disorganized, unclustered null model [16]. Well 195 known implementations of modularity optimization are 196 the Leiden and Louvain algorithms [17, 18]. Applied 197 directly to spike features, these established algorithms\n##########################################################################\n198 fail in a few different ways: 1) difficulty partitioning clus 199 ters with very different number of points; 2) very slow 200 processing speed for hundreds of thousands of points; 201 3) cannot use domain knowledge to make merge/split 202 decisions.bioRxiv preprint doi: https://doi.org/10.1101/2023.01.07.523036; this version posted January 7, 2023. The copyright holder for this preprint (which was not certified by peer review) is the author/funder, who ha ranted bioRxiv a license to display the preprint in perpetuity. It is made available under aCC-BY-NC 4.0 International license. \n##########################################################################\nTo remedy these problems, we developed a new graph-based algorithm, in which the clusters are defined as the stationary points of an iterative neighbor reassignment algorithm based on the modularity cost function (Figure 2a and see Methods). This method allowed us to find more of the small clusters compared to a straightforward application of the Leiden algorithm (Figure 2b). To improve the processing speed, which typically grows quadratically in the number of data points, we developed a landmark-based\n##########################################################################\nversion of the algorithm which uses nearest neighbors within a subset of all data points. The application of this algorithm resulted in oversplit clusters, which required additional merges using domain knowledge. To find the best merges efficiently, we used the modularity cost function to construct a “merging tree” (Figure 2c). Potential splits in this tree were tested using two criteria: 1) a bimodal distribution of spike projections along the regression axis between the two sub-clusters (Figure 2d, top),\n##########################################################################\nand 2) whether the cross-correlogram was refractory or not (Figure 2d, bottom).This clustering algorithm was applied to groups of spikes originating from the same $40~{\\upmu\\mathrm{m}}$ vertical section of the probe. After all sections were clustered, an additional merging step was performed which tested the refractoriness of the cross-correlogram for all pairs of templates with a correlation above 0.5, similar to the global merging step from previous versions $(2/2.5/3)$ . The final results are shown in (Figure 2e). Units that did not have a refractory period are shown grayed out in\n##########################################################################\n(Figure 2f); they likely correspond to neurons that were not well isolated. A quick overview of the units identified on this section of the probe shows that all units had refractory auto-correlograms, all pairs of clusters had bimodal projections on their respective regression axes, and all pairs of clusters had flat, non-refractory cross-correlograms (Figure 2h). These properties together indicate that these nine units correspond to nine distinct, well-isolated neurons. These clusters can also be\n##########################################################################\nvisualized on the probe, in their local contexts (Figure 2i).这是一个大标题:Electrical simulations with realistic drift \nTo test the performance of Kilosort4 and previous versions, we next developed a set of realistic simulations with different drift patterns. Constructing such a simulation requires knowledge of the dense electric fields of a neuron, because different drift levels sample the electric field at different positions. We obtained this knowledge by sampling neurons from recordings with large drift (Figure 3a) from a public repository of more than 500 Neuropixels recordings from the IBL consortium (Figure 3b). In this repository, we found 11 recordings with large, continuous drift that spanned over at least $40~{\\upmu\\mathrm{m}}$ , which is the spatial repetition period of a Neuropixels probe. We separately built two pools of units: one from neurons that were wellisolated and had refractory periods, and one from multi-unit activity which had refractory period contaminations. Drift levels were discretized in $2\\ \\upmu\\mathrm{m}$ intervals, and only units with enough spikes in each drift interval were considered. The average waveforms at five positions is shown for a few examples (Figure 3c and Figure S2a,b). To simulate drift, we generated a single average drift trace and additional deviations for each channel to account for heterogeneous drift. Spike trains were generated using shuffled inter-spike intervals from real units. For each simulation, a set of 600 ground-truth neurons were generated in this fashion, with amplitudes drawn from a truncated exponential distribution which matched the amplitudes in real datasets. Another 600 “multi-units” were added with lower amplitudes (Figure 3d). Additional independent noise was added on each channel. The resulting simulation was “un-whitened” across channels using a rotation matrix from real experiments (Figure S2c). \n\n##########################################################################\nTo test the performance of Kilosort4 and previous versions, we next developed a set of realistic simulations with different drift patterns. Constructing such a simulation requires knowledge of the dense electric fields of a neuron, because different drift levels sample the electric field at different positions. We obtained this knowledge by sampling neurons from recordings with large drift (Figure 3a) from a public repository of more than 500 Neuropixels recordings from the IBL consortium (Figure 3b). In\n##########################################################################\nthis repository, we found 11 recordings with large, continuous drift that spanned over at least $40~{\\upmu\\mathrm{m}}$ , which is the spatial repetition period of a Neuropixels probe. We separately built two pools of units: one from neurons that were wellisolated and had refractory periods, and one from multi-unit activity which had refractory period contaminations. Drift levels were discretized in $2\\ \\upmu\\mathrm{m}$ intervals, and only units with enough spikes in each drift interval were considered. The\n##########################################################################\naverage waveforms at five positions is shown for a few examples (Figure 3c and Figure S2a,b). To simulate drift, we generated a single average drift trace and additional deviations for each channel to account for heterogeneous drift. Spike trains were generated using shuffled inter-spike intervals from real units. For each simulation, a set of 600 ground-truth neurons were generated in this fashion, with amplitudes drawn from a truncated exponential distribution which matched the amplitudes in real\n##########################################################################\ndatasets. Another 600 “multi-units” were added with lower amplitudes (Figure 3d). Additional independent noise was added on each channel. The resulting simulation was “un-whitened” across channels using a rotation matrix from real experiments (Figure S2c).\n##########################################################################\nThis simulation framework allowed us to test many algorithms across many simulated experimental conditions [2, 3, 6, 19–23]. All algorithms other than Kilosort4 were run through their respective SpikeInterface wrappers to ensure consistent processing, and parameter adjustments were made in some cases to improve results (see Methods) [24]. The latest algorithm versions as of December 2022 were used in all cases, which are often substantially different from the initial published versions [2, 3]. Results for\n##########################################################################\nall conditions are shown in (Figure 3e-j) and quantified in Table 2. All the algorithms had reasonable run times (within 2x the duration of the simulations). The drift conditions we chose were based on patterns of drift identified in the IBL dataset (Figure S3): no drift, medium drift, high drift, fast drift and step drift. The medium drift condition was matched to the median recording from the IBL dataset. The high drift condition had a drift range spanning the entire $40\\upmu\\mathrm{m}$ spatial period of\n##########################################################################\nthe probe, thus sampling all potential shapes of each waveform. The fast drift condition uses drift on the timescale of seconds and sub-seconds, to simulate fast head movements such as during a behavioral task. The step drift condition simulates abrupt changes during an experiment, which are common in the IBL dataset and likely caused by excessive animal movements. This condition also simulates chronic recordings made on different days, where the probe is stationary on each day, but moves in-between days.\n##########################################################################\nSince this condition was the most difficult for all algorithms, we also tested whether an aligned sites probe configuration (such as in Neuropixels 2) improves the results.bioRxiv preprint doi: https://doi.org/10.1101/2023.01.07.523036; this version posted January 7, 2023. The copyright holder for this preprint (which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under aCC-BY-NC 4.0 International license. \n##########################################################################\n好的，我来解释一下这张图片。这似乎是一组关于深度漂移 (drift) 的模拟结果和实验数据分析图。\n\n**整体概况：**\n\n这张图主要研究了深度漂移在神经记录 (neural recording) 中的影响，通过模拟和实验数据，展示了不同漂移策略对信号的影响。\n\n**各个部分的解释：**\n\n*   **a (recording drift):** 展示了实际记录过程中的深度漂移随时间变化的曲线。它显示了记录深度随时间的变化趋势。\n*   **b (distribution of drift):** 呈现了深度漂移的分布直方图。它显示了漂移的范围和频率。\n*   **c (example drift):** 展示了两个具体神经记录的深度漂移轨迹图，以图形化方式展示了漂移的幅度。\n*   **d (simulation, noise):**  模拟的记录数据，加入了独立噪声。 帮助理解信号中的噪声成分\n*   **e (no drift):**  模拟的记录数据，没有深度漂移。\n*   **f (medium drift):**  模拟的记录数据，加入中等程度的深度漂移。\n*   **g (step drift):**  模拟的记录数据，深度漂移是阶梯式变化的。\n*   **h (fast drift):** 模拟的记录数据，加入快速的深度漂移。\n*   **i (step drift, aligned):** 展示与g图类似，但对时间做对齐处理的模拟结果。\n\n**总结:**\n\n这张图的目的是比较不同深度漂移策略对神经记录的影响，并研究如何补偿或减轻漂移带来的影响。 通过不同的模拟和对比，可以更好地理解漂移的本质及其对神经数据分析的影响。\n##########################################################################\n这张图片是一个表格，展示了不同的算法在不同\"漂移\" (drift) 程度下的运行时间，单位是分钟。\n\n**表格内容解读:**\n\n*   **左侧第一列:** 列出了不同的算法名称，包括 Kilsort4, Kilsort3, Kilsort2.5, Kilsort2, Kilsort, IronClust, MountainSort4, SpyKING CIRCUS, SpyKING CIRCUS 2, HDsort, Herding Spikes 和 Tridesclous2。\n*   **后几列 (no drift 到 step drift aligned):** 代表不同的\"漂移\"程度下算法的运行时间，表格中的数字表示运行时间（分钟），后跟±值表示误差范围。\n    *   **no drift:** 无漂移情况下的运行时间。\n    *   **medium drift:** 中等漂移情况下的运行时间。\n    *   **high drift:** 高漂移情况下的运行时间。\n    *   **fast drift:** 快速漂移情况下的运行时间。\n    *   **step drift:** 阶梯漂移情况下的运行时间。\n    *   **step drift aligned:** 阶梯漂移对齐后的运行时间。\n\n**总结:**\n\n该表格旨在比较不同算法在不同数据漂移程度下的效率。 不同的算法对不同漂移情况下的表现各不相同。  例如，Kilsort4在无漂移情况下的运行时间较短，但在高漂移情况下的运行时间较长。 \n\n希望这个解释能够帮助您理解这张表格！\n\n##########################################################################\nbioRxiv preprint doi: https://doi.org/10.1101/2023.01.07.523036; this version posted January 7, 2023. The copyright holder for this preprint (which was not certified by peer review) is the author/funder, who ha ranted bioRxiv a license to display the preprint in perpetuity. It is made available under aCC-BY-NC 4.0 International license. \n##########################################################################\n这是一个大标题:Benchmarks \nKilosort 2, 2.5, 3 and 4 outperformed all other algorithms in all cases. Kilosort 1 performed poorly, due to the lack of drift correction and its bias towards oversplit units. The nearest competing algorithm in performance was IronClust developed by the Flatiron Institute, which accounts for drift in a different way from Kilosort [19]. IronClust generally found $\\sim50\\%$ of all units, compared to the $80\\mathrm{-}90\\%$ found by Kilosort4 (Table 2). Many of the algorithms tested did not have explicit drift correction. Some of these (SpyKING CIRCUS, MountainSort4) matched the IronClust performance at no drift, medium and fast drift, but their performance deteriorated drastically with higher drift [2, 3]. Among all algorithms with explicit drift correction (Kilosort 2.5, 3 and 4), Kilosort4 consistently performed better due to its improved clustering algorithm, and in some cases performed much better (on the step drift conditions). As we suspected, the aligned sites condition recovered the full performance of Kilosort4 on the step drift simulations, likely because it reduces the vertical sampling from $40\\upmu\\mathrm{m}$ to $20~{\\upmu\\mathrm{m}}$ . \n\n##########################################################################\nKilosort 2, 2.5, 3 and 4 outperformed all other algorithms in all cases. Kilosort 1 performed poorly, due to the lack of drift correction and its bias towards oversplit units. The nearest competing algorithm in performance was IronClust developed by the Flatiron Institute, which accounts for drift in a different way from Kilosort [19]. IronClust generally found $\\sim50\\%$ of all units, compared to the $80\\mathrm{-}90\\%$ found by Kilosort4 (Table 2). Many of the algorithms tested did not have explicit drift\n##########################################################################\ncorrection. Some of these (SpyKING CIRCUS, MountainSort4) matched the IronClust performance at no drift, medium and fast drift, but their performance deteriorated drastically with higher drift [2, 3]. Among all algorithms with explicit drift correction (Kilosort 2.5, 3 and 4), Kilosort4 consistently performed better due to its improved clustering algorithm, and in some cases performed much better (on the step drift conditions). As we suspected, the aligned sites condition recovered the full performance of\n##########################################################################\nKilosort4 on the step drift simulations, likely because it reduces the vertical sampling from $40\\upmu\\mathrm{m}$ to $20~{\\upmu\\mathrm{m}}$ .We also tested how well the drift amplitudes were identified by the drift detection algorithm from Kilosort2.5 (in the Kilosort4 implementation) and found good performance in all cases, except for the fast drift condition where the timescale of drift was faster than the 2 sec bin size used for drift correction (Figure S4). Much smaller bin sizes cannot be used for drift estimation, since a minimum number of spike samples is required. Nonetheless, the results show that Kilosort still performed well in this\n##########################################################################\ncase, likely due to the robustness of the clustering algorithms. Finally, we calculated the performance of the algorithms as a function of the ground truth firing rates, amplitudes and spatial extents (Figure S5). The dependence of Kilosort4 on these variables was minimal. However, some of the other algorithms had a strong dependence on amplitude, which could not be improved by lowering spike detection thresholds. Also, many algorithms performed more poorly when the waveforms had a large spatial extent as\n##########################################################################\nopposed to having their electrical fields concentrated on just a few channels.这是一个大标题:Discussion \nHere we described Kilosort, a computational framework for spike sorting electrophysiological data. The latest version, Kilosort4, represents our cumulative development efforts over the past eight years, containing algorithms like template deconvolution (from Kilosort1), drift correction (from Kilosort2.5), as well as completely new clustering algorithms based on graph methods. Furthermore, Kilosort4 was re-written from the ground up in Python, an open-source programming language, using the pytorch package for GPU acceleration. The popularity of pytorch/python should ensure that Kilosort continues to be further improved and developed. We have also developed a new simulation framework to improve the benchmarking of spike sorting algorithms. Our simulations contain realistic background noise and realistic drift with diverse properties, and they are qualitatively similar to real recordings with Neuropixels probes. Kilosort4 outperformed all other algorithms on all simulation conditions, in some cases by a large margin. \n\n##########################################################################\nHere we described Kilosort, a computational framework for spike sorting electrophysiological data. The latest version, Kilosort4, represents our cumulative development efforts over the past eight years, containing algorithms like template deconvolution (from Kilosort1), drift correction (from Kilosort2.5), as well as completely new clustering algorithms based on graph methods. Furthermore, Kilosort4 was re-written from the ground up in Python, an open-source programming language, using the pytorch package\n##########################################################################\nfor GPU acceleration. The popularity of pytorch/python should ensure that Kilosort continues to be further improved and developed. We have also developed a new simulation framework to improve the benchmarking of spike sorting algorithms. Our simulations contain realistic background noise and realistic drift with diverse properties, and they are qualitatively similar to real recordings with Neuropixels probes. Kilosort4 outperformed all other algorithms on all simulation conditions, in some cases by a large\n##########################################################################\nmargin.\n##########################################################################\nAll versions of Kilosort have been developed primarily on Neuropixels data. However, since Kilosort adapts to the data statistics, it has been used widely on other types of probes and other recording methods. Some types of data do require special consideration. For example, some data cannot be drift corrected effectively due to either lacking a well-defined geometry (tetrodes), or due to the vertical spacing between electrodes being too high (more than $40~{\\upmu\\mathrm{m}})$ . This consideration also\n##########################################################################\napplies to data from single electrodes such as in a Utah array. Kilosort2 might be a better algorithm for such data, because it performs drift tracking without requiring an explicit channel geometry. Based on our benchmarks, Kilosort2 with drift tracking performs similarly to Kilosort2.5 with drift correction, except for the cases where step drift is present. Data from retinal arrays does not require drift correction and may be processed through Kilosort4, but it may require large amounts of GPU RAM for\n##########################################################################\narrays with thousands of electrodes and thus would be better split into multiple sections and processed separately. Another special type of data are cerebellar neurons with complex spikes, which can have variable, complex shapes that are not well matched by a single template, and specialized algorithms for detection may be required [25]. Another special type of recording comes from chronic experiments over multiple days, potentially separated by long intervals. While we have not explicitly tested such\n##########################################################################\nrecordings here, the benchmark results for the step drift simulation are encouraging because this simulation qualitatively matches changes we have seen chronically with implanted Neuropixels 2 electrodes [9].The problem of identifying neurons from extracellular recordings has a long history in neuroscience. The substantial progress seen in the past several years stems from multiple simultaneous developments: engineering of better devices (Neuropixels and others), better algorithms (Kilosort and others), improved visualizations of spike sorting results (Phy) and multiple rounds of user feedback provided by a quicklyexpanding community. Computational requirements have sometimes influenced the design of new\n##########################################################################\nprobes,bioRxiv preprint doi: https://doi.org/10.1101/2023.01.07.523036; this version posted January 7, 2023. The copyright holder for this preprint (which was not certified by peer review) is the author/funder, who h ranted bioRxiv a license to display the preprint in perpetuity. It is made available under aCC-BY-NC 4.0 International license. \n##########################################################################\nsuch as the aligned sites and reduced vertical spacing of Neuropixels 2 which were motivated by the need for better drift correction. Such computational considerations will hopefully continue to influence the development of future devices to increase the quality and quantity of neurons recovered by spike sorting. \n##########################################################################\n这是一个大标题:Acknowledgments \nThis research was funded by the Howard Hughes Medical Institute at the Janelia Research Campus. \n\nThis research was funded by the Howard Hughes Medical Institute at the Janelia Research Campus. \n\nThis research was funded by the Howard Hughes Medical Institute at the Janelia Research Campus. \n\nThis research was funded by the Howard Hughes Medical Institute at the Janelia Research Campus. \n\nThis research was funded by the Howard Hughes Medical Institute at the Janelia Research Campus. \n\n##########################################################################\nThis research was funded by the Howard Hughes Medical Institute at the Janelia Research Campus. \n##########################################################################\n这是一个大标题:Author contributions \nM.P. designed and built all versions of Kilosort. S.S. wrote the python GUI and C.S. developed the drifting simulations. C.S. and M.P. performed data analysis, coordinated the project and wrote the paper. \n\nM.P. designed and built all versions of Kilosort. S.S. wrote the python GUI and C.S. developed the drifting simulations. C.S. and M.P. performed data analysis, coordinated the project and wrote the paper. \n\nM.P. designed and built all versions of Kilosort. S.S. wrote the python GUI and C.S. developed the drifting simulations. C.S. and M.P. performed data analysis, coordinated the project and wrote the paper. \n\n##########################################################################\nM.P. designed and built all versions of Kilosort. S.S. wrote the python GUI and C.S. developed the drifting simulations. C.S. and M.P. performed data analysis, coordinated the project and wrote the paper. \n##########################################################################\n这是一个大标题:Code availability \nKilosort4 will be available upon publication at https: //www.github.com/mouseland/kilosort. Version 2, 2.5 and 3 are currently available at the same link. \n\nKilosort4 will be available upon publication at https: //www.github.com/mouseland/kilosort. Version 2, 2.5 and 3 are currently available at the same link. \n\nKilosort4 will be available upon publication at https: //www.github.com/mouseland/kilosort. Version 2, 2.5 and 3 are currently available at the same link. \n\nKilosort4 will be available upon publication at https: //www.github.com/mouseland/kilosort. Version 2, 2.5 and 3 are currently available at the same link. \n\n##########################################################################\nKilosort4 will be available upon publication at https: //www.github.com/mouseland/kilosort. Version 2, 2.5 and 3 are currently available at the same link. \n##########################################################################\n这是一个大标题:Data availability \nWe used datasets shared by Nick Steinmetz and the International Brain Laboratory [13, 15]. The datasets are available at at http: //data.cortexlab.net/singlePhase3/ and https://ibl.flatironinstitute.org/public/. \n\nWe used datasets shared by Nick Steinmetz and the International Brain Laboratory [13, 15]. The datasets are available at at http: //data.cortexlab.net/singlePhase3/ and https://ibl.flatironinstitute.org/public/. \n\nWe used datasets shared by Nick Steinmetz and the International Brain Laboratory [13, 15]. The datasets are available at at http: //data.cortexlab.net/singlePhase3/ and https://ibl.flatironinstitute.org/public/. \n\n##########################################################################\nWe used datasets shared by Nick Steinmetz and the International Brain Laboratory [13, 15]. The datasets are available at at http: //data.cortexlab.net/singlePhase3/ and https://ibl.flatironinstitute.org/public/. \n##########################################################################\n这是一个大标题:Methods \nThe Kilosort4 code library is implemented in Python 3 [10] using pytorch, numpy, scipy, scikit-learn, faiss-cpu , numba and tqdm [11, 26–32]. The graphical user interface additionally uses PyQt and pyqtgraph [12, 33]. The figures were made using matplotlib and jupyternotebook [34, 35]. Kilosort 2, 2.5 and 3 were implemented in MATLAB. \n\nThe Kilosort4 code library is implemented in Python 3 [10] using pytorch, numpy, scipy, scikit-learn, faiss-cpu , numba and tqdm [11, 26–32]. The graphical user interface additionally uses PyQt and pyqtgraph [12, 33]. The figures were made using matplotlib and jupyternotebook [34, 35]. Kilosort 2, 2.5 and 3 were implemented in MATLAB. \n\n##########################################################################\nThe Kilosort4 code library is implemented in Python 3 [10] using pytorch, numpy, scipy, scikit-learn, faiss-cpu , numba and tqdm [11, 26–32]. The graphical user interface additionally uses PyQt and pyqtgraph [12, 33]. The figures were made using matplotlib and jupyternotebook [34, 35]. Kilosort 2, 2.5 and 3 were implemented in MATLAB. \n##########################################################################\nWe demonstrate the Kilosort4 method stepby-step in Figure 1 and Figure 2. In Figure 1 an electrophysiological recording from Nick Steinmetz was used (”Single Phase 3”; [13] and https://figshare.com/articles/_Single_ Phase3_Neuropixels_Dataset/7666892). In Figure 2 an electrophysiological recording from the International Brain Laboratory was used (id: 6f6d2c8e28be-49f4-ae4d-06be2d3148c1; [15]). Both recordings were performed with a Neuropixels 1.0 probe, which has 384 sites organized in rows of two with a\n##########################################################################\nvertical spacing of $20~{\\upmu\\mathrm{m}}$ , a horizontal spacing of 32 $\\upmu\\mathrm{m}$ . Due to the staggered design ( $16~{\\upmu\\mathrm{m}}$ horizontal offset between consecutive rows), the spatial repetition period of this probe is $40\\upmu\\mathrm{m}$ .这是一个大标题:Graphical user interface (GUI) \nWe developed a graphical user interface to facilitate the user interaction with Kilosort4. This interface was built using pyqtgraph which itself uses PyQt [12, 33], and it replicates the Matlab GUI which was originally built for Kilosort2 by Nick Steinmetz. The GUI allows the user to select a data file, a configuration file for the probe, and set the most important parameters manually. In addition, a probe file can be constructed directly in the GUI. After loading the data and configuration file, the GUI displays a short segment of the data, which can be used to determine if the configuration was correct. Typical mistakes are easy to identify. For example if the total number of channels is incorrect, then the data will appear to be diagonally “streaked” because multi-channel patterns will be offset by 1 or 2 extra samples on each consecutive channel. Another typical problem is having an incorrect order of channels, in which case the user will see clear single-channel but no multi-channel waveforms. Finally, the GUI can produce several plots during runs which can be used to diagnose drift correction and the overall spike rates of the recording. \n\n##########################################################################\nWe developed a graphical user interface to facilitate the user interaction with Kilosort4. This interface was built using pyqtgraph which itself uses PyQt [12, 33], and it replicates the Matlab GUI which was originally built for Kilosort2 by Nick Steinmetz. The GUI allows the user to select a data file, a configuration file for the probe, and set the most important parameters manually. In addition, a probe file can be constructed directly in the GUI. After loading the data and configuration file, the GUI\n##########################################################################\ndisplays a short segment of the data, which can be used to determine if the configuration was correct. Typical mistakes are easy to identify. For example if the total number of channels is incorrect, then the data will appear to be diagonally “streaked” because multi-channel patterns will be offset by 1 or 2 extra samples on each consecutive channel. Another typical problem is having an incorrect order of channels, in which case the user will see clear single-channel but no multi-channel waveforms. Finally,\n##########################################################################\nthe GUI can produce several plots during runs which can be used to diagnose drift correction and the overall spike rates of the recording.这是一个大标题:Algorithms for Kilosort4 \nIn the next several sections we describe the algorithmic steps in Kilosort4. Some of these steps are inherited or evolved from previous versions. For clarity, we describe each of the steps exactly as they are currently used in Kilosort4. If a previous version of Kilosort is different, we clearly indicate the difference. We dedicate a completely separate section below for algorithms not used in Kilosort4 but used in previous versions. \n\nIn the next several sections we describe the algorithmic steps in Kilosort4. Some of these steps are inherited or evolved from previous versions. For clarity, we describe each of the steps exactly as they are currently used in Kilosort4. If a previous version of Kilosort is different, we clearly indicate the difference. We dedicate a completely separate section below for algorithms not used in Kilosort4 but used in previous versions. \n\n##########################################################################\nIn the next several sections we describe the algorithmic steps in Kilosort4. Some of these steps are inherited or evolved from previous versions. For clarity, we describe each of the steps exactly as they are currently used in Kilosort4. If a previous version of Kilosort is different, we clearly indicate the difference. We dedicate a completely separate section below for algorithms not used in Kilosort4 but used in previous versions. \n##########################################################################\nMany of the processing operations are performed on a per-batch basis. The default batch size is $N_{T}=$ $60,000$ , and it was $N_{T}=65,536$ in versions $2/2.5/3$ and $N_{T}=32,768$ in version 1. The increase in batch size in Kilosort2 was designed to allow better perbatch estimation of drift properties. Due to the perbatch application of temporal operations, we require special considerations at batch boundaries. Every batch of data is loaded with left and right padding of $n_{t}$ additional timepoints on\n##########################################################################\neach side $(n_{t}=61$ by default). On the first batch, the left pad consists of the first data sample repeated $n_{t}$ times. The last batch is typically less than a full batch size of $N_{T}$ . For consistency, we pad this batch to the full $N_{T}$ size using the repeated last value in the data.The clustering in Kilosort $_{3/4}$ is done in small $40\\upmu\\mathrm{m}$ sections of the probe, but including information from nearby channels and including spikes extracted at all timepoints. \n##########################################################################\nbioRxiv preprint doi: https://doi.org/10.1101/2023.01.07.523036; this version posted January 7, 2023. The copyright holder for this preprint (which was not certified by peer review) is the author/funder, who ha ranted bioRxiv a license to display the preprint in perpetuity. It is made available under aCC-BY-NC 4.0 International license. \n##########################################################################\n这是一个大标题:515 Preprocessing \nOur standard preprocessing pipeline includes a sequence of operations: common average referencing (CAR), temporal filtering, channel whitening and drift correction. In Kilosort4, all these steps are performed on demand whenever a batch of data is needed. In all previous versions, the preprocessing of the entire data was done first and the preprocessed data was stored in a separate binary file. Drift correction was introduced in Kilosort 2.5. \n\nOur standard preprocessing pipeline includes a sequence of operations: common average referencing (CAR), temporal filtering, channel whitening and drift correction. In Kilosort4, all these steps are performed on demand whenever a batch of data is needed. In all previous versions, the preprocessing of the entire data was done first and the preprocessed data was stored in a separate binary file. Drift correction was introduced in Kilosort 2.5. \n\n##########################################################################\nOur standard preprocessing pipeline includes a sequence of operations: common average referencing (CAR), temporal filtering, channel whitening and drift correction. In Kilosort4, all these steps are performed on demand whenever a batch of data is needed. In all previous versions, the preprocessing of the entire data was done first and the preprocessed data was stored in a separate binary file. Drift correction was introduced in Kilosort 2.5. \n##########################################################################\n这是一个大标题:525 Common average referencing \nThe first operations applied to data are to remove the mean across time for each batch, followed by removing the median across channels (common average referencing or CAR). The CAR can substantially reduce the impact of artifacts coming from remote sources such as room noise or optogenetics. The CAR must be applied before the other filtering and whitening operations, so that large artifacts do not ”leak” into other data samples. \n\nThe first operations applied to data are to remove the mean across time for each batch, followed by removing the median across channels (common average referencing or CAR). The CAR can substantially reduce the impact of artifacts coming from remote sources such as room noise or optogenetics. The CAR must be applied before the other filtering and whitening operations, so that large artifacts do not ”leak” into other data samples. \n\n##########################################################################\nThe first operations applied to data are to remove the mean across time for each batch, followed by removing the median across channels (common average referencing or CAR). The CAR can substantially reduce the impact of artifacts coming from remote sources such as room noise or optogenetics. The CAR must be applied before the other filtering and whitening operations, so that large artifacts do not ”leak” into other data samples. \n##########################################################################\n这是一个大标题:535 Temporal filtering \nThis is a per-channel filtering operation which defaults to a high-pass filter at $300\\mathsf{H z}$ . Bandpass filtering is typically done using IIR filters for example with Butterworth coefficients. Butterworth filters have some desirable properties in the frequency space, but their implementation on the GPU is slow. To accelerate it, we switch to using an FIR filter that simulates the Butterworth filter and we perform the FIR operation in FFT space taking advantage of the convolution theorem. To get the impulse response of a Butterworth filter, we simply filter a vector of size $N_{T}$ with all zeros and a single 1 value at position floor $(N_{T}/2)$ (0-indexed). \n\n##########################################################################\nThis is a per-channel filtering operation which defaults to a high-pass filter at $300\\mathsf{H z}$ . Bandpass filtering is typically done using IIR filters for example with Butterworth coefficients. Butterworth filters have some desirable properties in the frequency space, but their implementation on the GPU is slow. To accelerate it, we switch to using an FIR filter that simulates the Butterworth filter and we perform the FIR operation in FFT space taking advantage of the convolution theorem. To get the\n##########################################################################\nimpulse response of a Butterworth filter, we simply filter a vector of size $N_{T}$ with all zeros and a single 1 value at position floor $(N_{T}/2)$ (0-indexed).这是一个大标题:548 Channel whitening \n549 While temporal filtering reduces time-lagged corre  \n550 lations coming from background electrical activity, it   \n551 does not reduce across-channel correlations. To re  \n552 duce the impact of local sources, such as spikes from   \n553 $100{-}1000\\upmu m$ away from the probe, we perform chan  \n554 nel whitening in local neighborhoods of channels. A   \n555 separate whitening vector is estimated for each chan  \n556 nel based on its nearest 32 channels using the so  \n557 called ZCA transform, which stands for Zero Phase   \n558 Component Analysis [36]. ZCA is the data whitening   \n559 transformation which is closest in Euclidean norm to   \n560 the original data. For an $N$ by $T$ matrix $A$ , the ZCA   \n561 transform matrix $W$ is found by inverting the covari  \n562 ance matrix, using epsilon-smoothing of the singular   \n563 values: \n\n##########################################################################\n549 While temporal filtering reduces time-lagged corre 550 lations coming from background electrical activity, it 551 does not reduce across-channel correlations. To re 552 duce the impact of local sources, such as spikes from 553 $100{-}1000\\upmu m$ away from the probe, we perform chan 554 nel whitening in local neighborhoods of channels. A 555 separate whitening vector is estimated for each chan 556 nel based on its nearest 32 channels using the so 557 called ZCA transform, which stands for Zero Phase 558\n##########################################################################\nComponent Analysis [36]. ZCA is the data whitening 559 transformation which is closest in Euclidean norm to 560 the original data. For an $N$ by $T$ matrix $A$ , the ZCA 561 transform matrix $W$ is found by inverting the covari 562 ance matrix, using epsilon-smoothing of the singular 563 values:$$\n\\begin{array}{c}{C=\\mathsf{c o v}(A)}\\\\ {U,S,V=\\mathsf{s v d}(C)}\\\\ {W=U(S+\\mathsf{\\pmb{\\varepsilon}}I)^{-\\frac{1}{2}}U^{T}}\\end{array}\n$$The local whitening matrix $W$ is calculated separately for each channel and its neighborhood of 32 channels, and only the whitening vector corresponding to that channel is kept and embedded into a fullsize $N_{c h a n}$ by $N_{c h a n}$ matrix. This is preferable to directly calculating a grand $N_{c h a n}$ by $N_{c h a n}$ whitening matrix because it reduces the number of whitening coefficients to $32\\cdot N_{c h a n}$ instead of $N_{c h a n}\\cdot N_{c h a n}$ which prevents overfitting in the limit of a large $N_{c h a n}$ . \n##########################################################################\n这是一个大标题:573 Drift correction \nDrift correction is a complex preprocessing step which was described in detail in [9]. Here we only described a few small modifications in Kilosort4. The drift correction process can be separated into drift estimation and data alignment. In Kilosort4, drift estimation is performed in advance, while data alignment is performed on-demand along with the other preprocessing operations. Drift estimation includes a step of spike detection, which uses a set of predefined, “universal” templates to detect multi-channel spikes. In Kilosort 2.5 and 3, these predefined templates were constrained to be negative-going spikes, while in Kilosort4 we consider both positive and negative going spikes using pairs of inverted templates (for fast computation). Another modification in Kilosort4 is the use of linear interpolation for sampling the drift traces at every channel, in place of the “Makima” method used in previous versions. \n\n##########################################################################\nDrift correction is a complex preprocessing step which was described in detail in [9]. Here we only described a few small modifications in Kilosort4. The drift correction process can be separated into drift estimation and data alignment. In Kilosort4, drift estimation is performed in advance, while data alignment is performed on-demand along with the other preprocessing operations. Drift estimation includes a step of spike detection, which uses a set of predefined, “universal” templates to detect\n##########################################################################\nmulti-channel spikes. In Kilosort 2.5 and 3, these predefined templates were constrained to be negative-going spikes, while in Kilosort4 we consider both positive and negative going spikes using pairs of inverted templates (for fast computation). Another modification in Kilosort4 is the use of linear interpolation for sampling the drift traces at every channel, in place of the “Makima” method used in previous versions.Since data alignment is a linear operation performed with a Gaussian kriging kernel, it can be combined with channel whitening which is also a linear operation. In practical terms, the two $N_{c h a n}$ by $N_{c h a n}$ matrix multiplications are combined into one, thus further accelerating the computation. \n##########################################################################\n这是一个大标题:Template deconvolution \nTemplate deconvolution is the process of using a set of waveform templates matched to the data in order to detect spikes and extract their features, even when they overlap other spikes on the same channels and at the same timepoints. Template deconvolution can be seen as replacing the spike detection step in a classical spike sorting pipeline. The goal in Kilosort4 is to extract all the spikes above a certain waveform norm, and calculate their spike features in a way that discards \n\n##########################################################################\nTemplate deconvolution is the process of using a set of waveform templates matched to the data in order to detect spikes and extract their features, even when they overlap other spikes on the same channels and at the same timepoints. Template deconvolution can be seen as replacing the spike detection step in a classical spike sorting pipeline. The goal in Kilosort4 is to extract all the spikes above a certain waveform norm, and calculate their spike features in a way that discards \n##########################################################################\nbioRxiv preprint doi: https://doi.org/10.1101/2023.01.07.523036; this version posted January 7, 2023. The copyright holder for this preprint (which was not certified by peer review) is the author/funder, who h ranted bioRxiv a license to display the preprint in perpetuity. It is made available under aCC-BY-NC 4.0 International license. \n##########################################################################\n8 the contribution of nearby overlapping spikes. Tem  \n9 plate deconvolution improves on classical spike detec  \n10 tion in several ways: \n##########################################################################\n1) The detection of the spikes is performed by template matching, which is a more effective way of detecting spikes compared to threshold crossings, because it uses templates that represent the multi-channel spikes of the neurons being matched. \n##########################################################################\n2) Spikes that overlap in time and channels can be detected and extracted as separate events due to the use of iterative matching pursuit. Classical methods require an ”interdiction” area in time and channels around each detected spike where a second spike detection is disallowed, in order to prevent double detections of the same spike. \n##########################################################################\n3) The features extracted for each spike can be decontaminated from other overlapping spikes, due to the use of a generative or reconstructive model. As described below, these features are robust to imperfectly chosen templates. \n##########################################################################\n这是一个大标题:628 Template learning \nTo perform template deconvolution, a set of templates must be learned that can match all the detectable spikes on the probe. In previous Kilosort versions (1 / 2 / 2.5), special care was taken to ensure that these templates match neural waveforms on a one-to-one basis. This was necessary because relatively few additional merges and splits were performed after template deconvolution. In Kilosort 3 and 4, the templates do not need to match single neurons because the features extracted by template deconvolution are clustered again using more refined clustering algorithms. However, it is important that every spike in the raw data has some template to match to. \n\n##########################################################################\nTo perform template deconvolution, a set of templates must be learned that can match all the detectable spikes on the probe. In previous Kilosort versions (1 / 2 / 2.5), special care was taken to ensure that these templates match neural waveforms on a one-to-one basis. This was necessary because relatively few additional merges and splits were performed after template deconvolution. In Kilosort 3 and 4, the templates do not need to match single neurons because the features extracted by template\n##########################################################################\ndeconvolution are clustered again using more refined clustering algorithms. However, it is important that every spike in the raw data has some template to match to.To build a set of templates, we perform clustering on a set of spikes identified by template matching with a set of universal spike templates. This initial spike detection step is equivalent to the spike detection performed in Kilosort 2.5 for drift correction. The universal templates are defined by all possible combinations of 1) a spatial position in 2D; 2) a single-channel waveform shape; 3) a spatial size. The spatial positions need not be coincident with actual probe channels, and we choose them to\n##########################################################################\nupsample the channel densities by a factor of 2 in each dimension. For a Neuropixels 1 probe, this corresponds to 1536 positions. The single-channel waveform shapes are obtained by kmeans clustering of single channel spikes, either from a pre-existing dataset (IBL dataset) or from spikes detected by threshold crossings in the data, and we default to 6 such waveforms. Finally, the spatial sizes (five by default) define the envelope of an isotropic Gaussian centered on the spatial position of the template,\n##########################################################################\nwhich is used as per-channel amplitudes. In total, a set of 46,080 universal templates are used for a Neuropixels 1 probe; for more details see [9]. The spatial footprints are explicitly precomputed for all positions and all spatial sizes. The templates are effectively normalized to unit norm by separately normalizing the per-channel waveform templates and the spatial footprints. Since the universal templates are unit norm, their variance explained at each timepoint can be easily calculated as the dot\n##########################################################################\nproduct with the data, squared:\n##########################################################################\n$$\n\\begin{array}{r l}&{V_{\\mathrm{explained}}=\\|D\\|^{2}-\\mathsf{m i n}_{x}\\|D-x W\\|^{2}}\\\\ &{\\qquad=\\|D\\|^{2}-\\|D-(W^{T}D)W\\|^{2}}\\\\ &{\\qquad=(W^{T}D)^{2}}\\end{array}\n$$where $W$ is the unit-norm universal template, $D$ is the data over a particular set of channels and timepoints, and $x$ is the best matching amplitude that the template needs to be multiplied by to match the data. \n##########################################################################\nThe dot products between each of these templates and the data at each timepoint can be performed efficiently in the following order: 1) temporal convolution of each data channel with each of the 6 single-channel waveforms; 2) per timepoint matrix multiplication with a set of weights corresponding to all positions and all spatial sizes. Once the dot products are calculated in this manner, the largest variance explained value is kept at each spatial position of each template. For a Neuropixels probe, this is\n##########################################################################\na matrix of size 1536 by $N_{T}$ (batch size). The goal of this spike detection step is to find localized peaks in this matrix, which must be local maxima in a neighborhood of timepoints $(\\pm20)$ and spatial positions (100 nearest positions). The relatively large neighborhood size ensures that no spike is detected twice, but prevents many overlapping spikes from being detected (typically about $50\\%$ of spikes go undetected). However, the missing spikes are not a concern for the purpose of template\n##########################################################################\nlearning, since it is extremely unlikely that all the spikes from a neuron will be consistently missed by this procedure.Once the spikes are detected, we extract PC features in the 10 nearest channels to each detection. We use a set of six PCs which are found either from a preexisting dataset (IBL dataset) or from spikes detected by threshold crossings. For each spike, an XY position on the probe is computed based on the center-of-mass across channels of the spike’s projection on the bestmatching single channel template (same as in Kilosort 2.5). We assign all spikes in $40~{\\upmu\\mathrm{m}}$ bins according to their vertical\n##########################################################################\nposition, and embed all spikes detected in the same bin to the same set of channels (which isbioRxiv preprint doi: https://doi.org/10.1101/2023.01.07.523036; this version posted January 7, 2023. The copyright holder for this preprint (which was not certified by peer review) is the author/funder, who ha ranted bioRxiv a license to display the preprint in perpetuity. It is made available under aCC-BY-NC 4.0 International license. \n##########################################################################\n08 usually more than 10 channels due to differences be 709 tween spike positions). Finally, the embedded PC fea 10 tures are clustered according to the same graph-based 11 clustering algorithm we describe below, using only the 12 merging criterion of the bimodal regression-axis and 13 not using the cross-correlation based criterion. In Kilo 14 sort3, the same procedure is applied but the clustering 15 algorithm is recursive pursuit. After clustering each 40 16 $\\upmu\\mathrm{m}$ section of the probe, the\n##########################################################################\ncentroids are multiplied 17 back from PC space into spatio-temporal waveforms, 18 and pooled together across the probe.Templates from the same neuron may be detected multiple times, either on the same $40~{\\upmu\\mathrm{m}}$ section or in nearby sections. This is not inherently a problem because each neuron can have multiple templates. However, it can become a problem if these multiple templates are not aligned to each other, because then spikes from the same neuron will be detected at different temporal positions, which changes their PC feature distribution. In addition, having many templates makes the spike detection step\n##########################################################################\nmemory and compute inefficient. A solution to both these problems is to merge together templates which have a high correlation with each other and similar means, where the correlation is maximized across possible timelags. In addition, we temporally align all templates based on their maximal correlation with the same six prototypical singlechannel waveforms describe above. Note that this merging step may result in the opposite scenario of having one template for multiple neurons. This is also not a problem,\n##########################################################################\nbecause templates are only merged when they have a high correlation, and thus the same average template can successfully match the shape of multiple neurons.这是一个大标题:Spike detection with learned templates and matching pursuit \nOnce a set of templates is learned, they can be used for template matching similar to the universal templates described above. The main difference is that instead of allowing for an arbitrary scaling factor $x$ , we require that matches use the average amplitude of the template it was found with. The variance explained of learned template $W$ of some data $D$ thus becomes: \n\nOnce a set of templates is learned, they can be used for template matching similar to the universal templates described above. The main difference is that instead of allowing for an arbitrary scaling factor $x$ , we require that matches use the average amplitude of the template it was found with. The variance explained of learned template $W$ of some data $D$ thus becomes: \n\n##########################################################################\nOnce a set of templates is learned, they can be used for template matching similar to the universal templates described above. The main difference is that instead of allowing for an arbitrary scaling factor $x$ , we require that matches use the average amplitude of the template it was found with. The variance explained of learned template $W$ of some data $D$ thus becomes: \n##########################################################################\n$$\n\\begin{array}{r l}&{V_{\\mathrm{explained}}=\\|D\\|^{2}-\\mathsf{m i n}_{x}\\|D-x_{W}W\\|^{2}}\\\\ &{\\qquad=2x_{W}W^{T}D-x_{W}^{2}}\\end{array}\n$$751 Like before, this quantity only requires the calcula  \n752 tion of $W^{T}D$ , which can be done convolutionally for   \n753 each template. In practice, we represent templates us\n##########################################################################\ning a three-rank approximation, factorized over channels and time, which speeds up the convolutions dramatically [6]. We first multiply the data with the channel weights for each rank, and convolve the resulting traces with the temporal components. The threerank approximation captures nearly the entire waveform variance in all cases ([6]), and also helps to denoise templates calculated from relatively few spikes. \n##########################################################################\nTo extract overlapping spikes, we must detect spikes iteratively over the same portion of data, and subtract off from the data those parts attributed to spike detections. This subtraction allows for another pass of detections to be performed, which can detect other spikes left over and yet un-subtracted. This procedure is called matching pursuit ([37]) and is fundamentally a sequential process: to detect another spike, one must first subtract off the contributions of spikes detected before. However, we can\n##########################################################################\nparallelize this step thus making it suitable for GPU processing by observing that the subtraction of a single spike results in highly-localized changes to the data, which cannot affect the calculated amplitudes far from the position of that subtracted spike. Thus, we can detect and subtract multiple spikes in one round as long as they are far enough from each other. Upon calculating a matrix of variance explained for each template at each timepoint, we detect peaks in this matrix which are local maxima\n##########################################################################\nover local neighborhoods in time $\\pm n_{t}$ time samples, and across all channels. After detection, the optimal amplitude for each spike is calculated and its contribution from the data is subtracted off. To avoid recalculating the dot products of templates at all timepoints, the contribution of the subtracted spikes to the dot-products is directly updated locally using a set of precomputed dot-products between templates, at all possible timelags. This detection and subtraction process is repeated for 50\n##########################################################################\nrounds, with later rounds being much faster due to the increasingly smaller number of spikes left to extract.这是一个大标题:Extracting PC features with background subtraction \nThe final step in template deconvolution is to extract features from the data to be used by the clustering algorithm. One possibility would be to directly extract PC features from the preprocessed data, at the spike detection times (Figure 1h), however this results in contamination with background spikes. A better option is to first subtract the effect of other spikes, since we know from the matching pursuit step how much these other spikes contribute (Figure 1e). To do this computation efficiently, we first extract PC features from the residual (Figure 1f), and then add back to these features the contribution of the template which was used to extract the spike. The contribution of each template \n\n##########################################################################\nThe final step in template deconvolution is to extract features from the data to be used by the clustering algorithm. One possibility would be to directly extract PC features from the preprocessed data, at the spike detection times (Figure 1h), however this results in contamination with background spikes. A better option is to first subtract the effect of other spikes, since we know from the matching pursuit step how much these other spikes contribute (Figure 1e). To do this computation efficiently, we\n##########################################################################\nfirst extract PC features from the residual (Figure 1f), and then add back to these features the contribution of the template which was used to extract the spike. The contribution of each templatebioRxiv preprint doi: https://doi.org/10.1101/2023.01.07.523036; this version posted January 7, 2023. The copyright holder for this preprint (which was not certified by peer review) is the author/funder, who ha ranted bioRxiv a license to display the preprint in perpetuity. It is made available under aCC-BY-NC 4.0 International license. \n##########################################################################\n807 in PC space is precomputed for faster processing. \n##########################################################################\n这是一个大标题:Graph-based clustering \nThe new clustering algorithm in Kilosort4 uses graphbased algorithms. This class of algorithms relies entirely on the graph constructed by finding the nearest neighbors to each data point. There are several steps: \n\nThe new clustering algorithm in Kilosort4 uses graphbased algorithms. This class of algorithms relies entirely on the graph constructed by finding the nearest neighbors to each data point. There are several steps: \n\nThe new clustering algorithm in Kilosort4 uses graphbased algorithms. This class of algorithms relies entirely on the graph constructed by finding the nearest neighbors to each data point. There are several steps: \n\n##########################################################################\nThe new clustering algorithm in Kilosort4 uses graphbased algorithms. This class of algorithms relies entirely on the graph constructed by finding the nearest neighbors to each data point. There are several steps: \n##########################################################################\n1) Neighbor finding with subsampling   \n2) Iterative neighbor reassignment   \n3) Hierarchical linkage tree \n##########################################################################\n这是一个大标题:816 Neighbor finding with subsampling \nMany frameworks for fast neighbor finding exist and we tested a lot of them for spike sorting data. In the end, the brute force implementation from the faiss framework [38] outperformed other approaches in speed on modern multi-core computers for the range of data points we need to search over (10,000-100,000) and the number of data points we need to find neighbors for (100,000-1,000,000). \n\nMany frameworks for fast neighbor finding exist and we tested a lot of them for spike sorting data. In the end, the brute force implementation from the faiss framework [38] outperformed other approaches in speed on modern multi-core computers for the range of data points we need to search over (10,000-100,000) and the number of data points we need to find neighbors for (100,000-1,000,000). \n\n##########################################################################\nMany frameworks for fast neighbor finding exist and we tested a lot of them for spike sorting data. In the end, the brute force implementation from the faiss framework [38] outperformed other approaches in speed on modern multi-core computers for the range of data points we need to search over (10,000-100,000) and the number of data points we need to find neighbors for (100,000-1,000,000). \n##########################################################################\n这是一个大标题:5 Iterative neighbor assignment \nClustering algorithms based on graphs typically optimize a cost function such as the modularity cost function. We review this approach first, before describing our new approach. Following [17], the modularity cost function is defined by \n\nClustering algorithms based on graphs typically optimize a cost function such as the modularity cost function. We review this approach first, before describing our new approach. Following [17], the modularity cost function is defined by \n\n##########################################################################\nClustering algorithms based on graphs typically optimize a cost function such as the modularity cost function. We review this approach first, before describing our new approach. Following [17], the modularity cost function is defined by \n##########################################################################\n$$\n{\\mathcal{H}}={\\frac{1}{2m}}\\sum_{c}\\left(e_{c}-\\gamma{\\frac{K_{c}^{2}}{2m}}\\right)\n$$where m is the total number of edges in the graph, $e_{c}$ is the number of edges in community $c$ , $K_{c}$ is the sum of degrees in community $c$ and $\\upgamma$ is a “resolution” parameter that controls the number of clusters. The 2Kcm2 can be interpreted as the expected number of edges in community $c$ from a null model with the same node degrees as the data but otherwise random graph connections. \n##########################################################################\nSpecialized optimization algorithms exist to maximize the modularity cost function by moving nodes between communities and performing merges when the node re-assignment converges [18]. Additionally, splitting steps and other optimizations were recently introduced which improve the results of the algorithm and its speed [17]. These algorithms are effective for many types of data, yet have a substantial failure mode for spike sorting data: they have difficulty clustering data with very different number of\n##########################################################################\npoints per cluster. In practice, for our clustering problems, there are often very large clusters of up to 100,000 points together with clusters with many fewer $(<1,000)$ points. A low resolution parameter $\\upgamma$ can keep the large cluster in one piece, but also merges the small clusters into larger clusters. Conversely, high resolution parameters may return the small clusters as individual clusters, but can split the large cluster into very many (hundreds) of pieces. The oversplitting is not\n##########################################################################\ninherently a bad property, since we will perform merges on these clusters anyway, but the very large number of pieces returned for the large clusters means that very many correct merging decisions must be made, which is in itself a very difficult optimization problem. In addition, running the Louvain/Leiden algorithms with large resolution parameters may somewhat reduce the effectiveness of the algorithm since the community penalty γ 2Kcm only has a null model interpretation for $\\lambda=1$ .\n##########################################################################\nTo improve on these algorithms, we started from the observation that local minima of the neighbor re-assignment step have some desirable properties. These local minima arise because the neighbor reassignment step monotonically improves the modularity cost function by greedily moving nodes to new clusters if that improves the modularity score. This step converges after a while, because no more clusters can be moved. This is however a local minimum of the optimization, and the modularity can often be further\n##########################################################################\nincreased by making merges between clusters. Unlike the node re-assignment, which consists of small local moves, the merging between clusters is a global move in the cost function and can thus escape the local minimum. Algorithms like Leiden/Louvain take advantage of such global merges by applying the node re-assignment step again on a new graph made by aggregating all the points into their clusters when the local minimum is reached.Our observation was that the local minima themselves can consist of good clustering, if the neighbor re-assignment step is initialized appropriately. Our initialization uses the K-means $^{++}$ algorithm to partition the data initially into 200 clusters [14]. The node reassignment algorithm for the modularity cost function with $\\gamma=1$ is run for a fixed number of iterations (typically sufficient for convergence). The converged partitioning of the data is then used as a clustering result. Especially\n##########################################################################\nrelevant to the next step, the algorithm almost never made incorrect merges, and instead output some clusters oversplit. This bias towards oversplitting is important, because it allows us to correct the mistakes of the algorithm by making correct merge decisions, which is much easier than finding the correct split in a cluster.We also found that clusters which were oversplit generally had a reason to be oversplit: the separate pieces identified by the algorithm were in fact sufficiently different to create a local minimum in the clus\n##########################################################################\nbioRxiv preprint doi: https://doi.org/10.1101/2023.01.07.523036; this version posted January 7, 2023. The copyright holder for this preprint (which was not certified by peer review) is the author/funder, who ha nted bioRxiv a license to display the preprint in perpetuity. It is made available under aCC-BY-NC 4.0 International license. \n##########################################################################\n906 ter assignments. This is a common problem in spike 907 sorting data, where nonlinear changes in the waveform 908 can result in clusters that appear bimodal in Euclidian 909 space. An extreme example of this effect is due to 910 abrupt drifts of the probe changing the sampling of the 911 waveforms by a non-integer multiple of the probe pe 912 riod. Even after drift correction, waveforms sampled 913 at the two different positions will be much more similar 914 to other waveforms from the same position,\n##########################################################################\nthan they 915 are to waveforms sampled at the other position (Fig 916 ure S2b). As a consequence, many algorithms return 917 such units oversplit into two halves, as can be clearly 918 seen in the benchmark results for the step drift condi 919 tion, where many units are identified with exactly a 0.5 920 score, which corresponds to $50\\%$ of the spikes identi 921 fied.这是一个大标题:922 Hierarchical merging tree \nTo perform merges, we could take two strategies: 1) a brute-force approach in which we check all pairs of clusters for merges, or at least the ones with high waveform correlation; 2) a directed approach where we use the structure of the data to tell us which merges to check. We use both, starting with the second one to reduce the number of clusters and thus reduce the number of brute-force checks we need to make later. \n\nTo perform merges, we could take two strategies: 1) a brute-force approach in which we check all pairs of clusters for merges, or at least the ones with high waveform correlation; 2) a directed approach where we use the structure of the data to tell us which merges to check. We use both, starting with the second one to reduce the number of clusters and thus reduce the number of brute-force checks we need to make later. \n\n##########################################################################\nTo perform merges, we could take two strategies: 1) a brute-force approach in which we check all pairs of clusters for merges, or at least the ones with high waveform correlation; 2) a directed approach where we use the structure of the data to tell us which merges to check. We use both, starting with the second one to reduce the number of clusters and thus reduce the number of brute-force checks we need to make later. \n##########################################################################\nFor the directed approach, we construct a hierarchical merging tree based on the modularity cost function. The leaves of this tree consist of the clusters identified at the previous step. For each pair of clusters $i,j$ , we aggregate the neighbors and node degrees, similar to the Leiden/Louvain algorithms, thus resulting in a full matrix $K$ of size $n_{k}$ by $n_{k}$ where $n_{k}$ is the number of clusters, and where $K_{i j}$ is the number of edges between clusters $i,j$ , while $K_{i i}$ is the number\n##########################################################################\nof internal edges. Additionally, a variable $k_{i}$ holds the aggregated degree of each cluster $i$ . The linkage tree is constructed by varying the resolution parameter $\\gamma$ in the modularity cost function from $\\infty$ down to 0. As γ decreases, merges of two clusters start to increase the modularity cost function. Specifically, a pair of clusters gets merged when the modularity $\\mathcal{H}_{2}$ after merging equals the modularity $\\mathcal{H}_{1}$ before merging, where:$$\n\\begin{array}{r l r}&{}&{\\mathcal{H}_{1}=\\left(K_{i i}-\\gamma\\frac{k_{i}^{2}}{2m}\\right)+\\left(K_{j j}-\\gamma\\frac{k_{j}^{2}}{2m}\\right)+\\mathrm{constant}}\\\\ &{}&{\\mathcal{H}_{2}=\\left(K_{i j}+K_{i i}+K_{j j}-\\gamma\\frac{(k_{i}+k_{j})^{2}}{2m}\\right)+\\mathrm{constant}}\\end{array}\n$$\n$$\n\\begin{array}{c}{\\displaystyle{\\mathcal{H}_{2}-\\mathcal{H}_{1}=K_{i j}-\\hat{\\gamma}_{i j}\\frac{k_{i}k_{j}}{2m}=0}}\\\\ {\\displaystyle{\\hat{\\gamma}_{i j}=\\frac{2m K_{i j}}{k_{i}k_{j}}}}\\end{array}\n$$\nIn other words, a pair of clusters $i,j$ should be merged when $\\upgamma$ reaches a value of $2m K_{i j}/(k_{i}k_{j})$ . After merging, the matrix $K$ and vector $k$ can be recomputed with the two clusters $i,j$ becoming aggregated into one. Note that a merging decision does not change the $\\hat{\\boldsymbol{\\upgamma}}$ for other pairs of clusters, and it cannot result in a higher $\\hat{\\boldsymbol{\\upgamma}}$ than the current $\\hat{\\gamma}_{i j}$ . This can be shown by reductio ad absurdum: if the merged $i,j$ cluster had a higher $\\hat{\\boldsymbol{\\upgamma}}$ with another cluster $l$ , it would imply that one of the original clusters $i$ or $j$ had a higher $\\hat{\\upgamma}_{i l}$ or $\\hat{\\boldsymbol{\\upgamma}}_{j l}$ , and thus it should have been merged a priori. The monotonic property of $\\hat{\\gamma}_{i j}$ ensures that a well-defined merging tree exists, with a strictly decreasing sequence of $\\hat{\\boldsymbol{\\upgamma}}$ for increasingly higher merges in the tree. Empirically, we have found that the resulting merging tree is very useful for making merge/split decisions. \n##########################################################################\n这是一个大标题:Split/merge criteria \nWith the tree constructed, we next move down the tree starting from the top and make individual merge/split decisions at every node. If a node is not being split, then the splits below that node are no longer checked. We use two splitting criteria: 1) the bimodality of the data projection along the regression axis between the two clusters and 2) the degree of refractoriness of the cross-correlogram. If the pair of units has a refractory cross-correlogram, then the split is always performed. If the cross-correlogram is not refractory, then the split is performed if and only if the projection along the regression axis is bimodal. \n\n##########################################################################\nWith the tree constructed, we next move down the tree starting from the top and make individual merge/split decisions at every node. If a node is not being split, then the splits below that node are no longer checked. We use two splitting criteria: 1) the bimodality of the data projection along the regression axis between the two clusters and 2) the degree of refractoriness of the cross-correlogram. If the pair of units has a refractory cross-correlogram, then the split is always performed. If the\n##########################################################################\ncross-correlogram is not refractory, then the split is performed if and only if the projection along the regression axis is bimodal.这是一个大标题:Bimodality of regression axis \nConsider a set of spike features $\\mathbf{x}_{k}$ with associated labels $y_{k}\\in\\{-1,1\\}$ , where $^{-1}$ indicates the first cluster and 1 indicates the second cluster. A regression axis ˆu can be obtained by minimizing: \n\nConsider a set of spike features $\\mathbf{x}_{k}$ with associated labels $y_{k}\\in\\{-1,1\\}$ , where $^{-1}$ indicates the first cluster and 1 indicates the second cluster. A regression axis ˆu can be obtained by minimizing: \n\nConsider a set of spike features $\\mathbf{x}_{k}$ with associated labels $y_{k}\\in\\{-1,1\\}$ , where $^{-1}$ indicates the first cluster and 1 indicates the second cluster. A regression axis ˆu can be obtained by minimizing: \n\n##########################################################################\nConsider a set of spike features $\\mathbf{x}_{k}$ with associated labels $y_{k}\\in\\{-1,1\\}$ , where $^{-1}$ indicates the first cluster and 1 indicates the second cluster. A regression axis ˆu can be obtained by minimizing: \n##########################################################################\n$$\n\\hat{\\mathbf{u}}=\\mathsf{a r g m i n}_{u}\\sum_{k}\\left(\\mathbf{u}^{T}\\mathbf{x}_{k}-y_{k}\\right)^{2}\n$$This regression problem becomes highly unbalanced when one of the clusters has many more points than the other. We therefore add a set of weights 6 $w_{-1}=n_{2}/(n_{1}+n_{2}),w_{+1}=n_{1}/(n_{1}+n_{2})$ , where $n_{1}$ , $n_{2}$ are the number of spikes in the first and second 8 cluster. \n##########################################################################\nbioRxiv preprint doi: https://doi.org/10.1101/2023.01.07.523036; this version posted January 7, 2023. The copyright holder for this preprint (which was not certified by peer review) is the author/funder, who ha ranted bioRxiv a license to display the preprint in perpetuity. It is made available under aCC-BY-NC 4.0 International license. \n##########################################################################\n$$\n\\hat{\\mathbf{u}}=\\mathsf{a r g m i n}_{u}\\sum_{k}w_{y_{k}}\\left(\\mathbf{u}^{T}\\mathbf{x}_{k}-y_{k}\\right)^{2}\n$$This weighted regression problem can be solved in the usual fashion. Finally, we use the ˆu axis to estimate how well separated the clusters are by projecting $x_{p r o j}=\\hat{\\mathbf{u}}^{T}\\mathbf{x}_{k}$ . The projections are binned in 400 bins linearly-spaced between $^{-2}$ and 2, and the histogram is gaussian smoothed with a standard deviation of 4 bins. To score the degree of bimodality, we find three important values in the histogram: the peak of the negative portion, the trough around 0, and the peak of the positive portion. First we find the trough $x_{m i n}$ at position $i_{m i n}$ in the bin range of 175 to 225. Then we find the peaks $x_{1}$ , $x_{2}$ in the bin ranges from 0 to $i_{m i n}$ and from $i_{m i n}$ to 400. The bimodality score is defined by \n##########################################################################\n1027 0.5 sec. We consider the central bins of the cross 1028 correlograms, and calculate how likely it is to see 1029 a very small number of coincidences in that bin, if 1030 the two clusters are from neurons firing independently 1031 from each other. We define $n_{k}$ as the number of coin 1032 cidences in the central $-k$ to $+k$ bin range, $R$ as the 1033 baseline rate of coincidences calculated from the other 1034 bins of the cross-correlogram. Cross-correlograms 1035 may be assymetric, and to account\n##########################################################################\nfor that we esti 1036 mate $R$ as the maximum rate from either the left or 1037 right shoulder of the cross-correlogram. We use two 1038 criteria to determine refractoriness. The first criterion 1039 is simply based on the ratio of refractory coincidences 1040 versus coincidences in other bins which works well in 1041 most cases, except when one of the units has very few 1042 spikes, in which case very few refractory coicindences 1043 may be observed just by chance. For the first criterion, 1044 we use the\n##########################################################################\nratio $R_{12}$ of $n_{k}$ to its expected value from a 1045 rate $R$ , where $R_{12}$ takes the minimum value of this ratio 1046 across $k$ . We set a threshold of 0.25 on $R_{12}$ to consider 1047 a CCG refractory, and 0.1 to consider an ACG refrac 1048 tory. For the second criterion, we use the probability 1049 $p_{k}$ that $n_{k}$ spikes or less would be observed from a 1050 Poisson process with rate $\\lambda_{k}=(2k+1)R$ , which we 1051 approximate using a Gaussian with the same mean 1052 and standard\n##########################################################################\ndeviation as the Poisson process as$$\n\\mathsf{b i m o d}=1-\\mathsf{m a x}\\big(x_{m i n}/x_{1},x_{m i n}/x_{2}\\big)\n$$In other words, we compare the density of the $x_{p r o j}$ distribution at its trough to the peak densities for both clusters. If the density at the trough is similar in value to the density of either the left or right peak, that indicates a non-bimodal distribution."
  }
}